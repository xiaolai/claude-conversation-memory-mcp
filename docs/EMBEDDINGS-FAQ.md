# Embeddings FAQ - How Automatic Embedding Generation Works

## â“ "How do users generate embeddings?"

**Answer**: **Embeddings are generated AUTOMATICALLY during indexing!**

When you run:
```
User: "Index my conversation history"
Claude uses: index_conversations tool
```

The system automatically:
1. âœ… Parses conversations
2. âœ… Extracts decisions, mistakes, requirements
3. âœ… **Generates embeddings for semantic search** â† AUTOMATIC!
4. âœ… Stores everything in database

---

## ğŸ¤” "What's the Ollama model used for?"

**Answer**: **Ollama (mxbai-embed-large) generates the embeddings for semantic search!**

### Embedding Flow:

```
User message â†’ index_conversations
              â†“
         Parse conversations
              â†“
         Extract text content
              â†“
     ğŸ¯ Generate embeddings â† Ollama/Transformers/OpenAI
              â†“
         Store in vec tables
              â†“
     Enable semantic search âœ…
```

---

## ğŸš€ Automatic Provider Selection

The system automatically chooses the best available provider:

### 1. **Try Ollama First** (Default)
```
Check: Is Ollama running at http://localhost:11434?
Check: Is mxbai-embed-large model available?
If YES â†’ Use Ollama (fast, high-quality, local)
```

### 2. **Fallback to Transformers.js** (Fully Offline)
```
If Ollama unavailable â†’ Use @xenova/transformers
Model: Xenova/all-MiniLM-L6-v2 (384 dimensions)
No internet required!
```

### 3. **Fallback to OpenAI** (If configured)
```
If OPENAI_API_KEY set â†’ Use OpenAI embeddings
Model: text-embedding-3-small (1536 dimensions)
Highest quality, requires API key
```

### 4. **Final Fallback: Full-Text Search**
```
If all embedding providers fail â†’ Use SQLite FTS
Still works! Just no semantic search
Can upgrade later by re-indexing
```

---

## âœ… Embedding Status in Responses

**NEW in v0.2.0**: The `index_conversations` tool now reports embedding status!

### Success Response:
```json
{
  "success": true,
  "embeddings_generated": true,
  "message": "Indexed 3421 messages\nâœ… Semantic search enabled (embeddings generated)"
}
```

### Failure Response (with fallback):
```json
{
  "success": true,
  "embeddings_generated": false,
  "embedding_error": "Ollama not running, Transformers.js not installed",
  "message": "Indexed 3421 messages\nâš ï¸ Semantic search unavailable: ...\n   Falling back to full-text search"
}
```

---

## ğŸ”§ Common Scenarios

### Scenario 1: Fresh Install (No Setup)

```bash
# User installs MCP
npx claude-conversation-memory-mcp

# User indexes conversations
User: "Index my conversations"

# What happens:
âœ… Indexing succeeds
âœ… Transformers.js downloads model automatically (~100MB)
âœ… Embeddings generated (384 dimensions)
âœ… Semantic search works!
```

**No configuration needed!** Works out of the box.

---

### Scenario 2: Ollama User (Best Experience)

```bash
# User has Ollama installed
ollama pull mxbai-embed-large

# User indexes conversations
User: "Index my conversations"

# What happens:
âœ… Detects Ollama running
âœ… Uses mxbai-embed-large (1024 dimensions)
âœ… Fast, high-quality embeddings
âœ… Semantic search works great!
```

---

### Scenario 3: OpenAI User (Premium Quality)

```bash
# User sets API key
export OPENAI_API_KEY=sk-...

# User indexes conversations
User: "Index my conversations"

# What happens:
âœ… Detects OpenAI API key
âœ… Uses text-embedding-3-small (1536 dimensions)
âœ… Highest quality embeddings
âœ… Costs ~$0.02 per 1M tokens
```

---

### Scenario 4: Offline / No Embeddings

```bash
# No Ollama, no internet, Transformers.js fails

# User indexes conversations
User: "Index my conversations"

# What happens:
âœ… Indexing still succeeds!
âš ï¸ No embeddings generated
âš ï¸ Message: "Semantic search unavailable, falling back to FTS"
âœ… Full-text search still works
âœ… Can re-index later to add embeddings
```

---

## ğŸ¯ How Semantic Search Works

### With Embeddings:
```
User: "What did we discuss about authentication?"
      â†“
Convert query to embedding (vector)
      â†“
Find similar embeddings (cosine similarity)
      â†“
Return relevant conversations âœ…
```

### Without Embeddings (FTS Fallback):
```
User: "What did we discuss about authentication?"
      â†“
Search for keyword "authentication"
      â†“
Return exact matches only
      â†“
Still works! Just less smart
```

---

## ğŸ“Š Embedding Statistics

After indexing, check what was generated:

```sql
-- How many embeddings were created?
SELECT COUNT(*) FROM message_embeddings;

-- What dimensions are they?
SELECT
  CASE
    WHEN LENGTH(embedding) / 4 = 384 THEN 'Transformers.js'
    WHEN LENGTH(embedding) / 4 = 768 THEN 'nomic-embed-text'
    WHEN LENGTH(embedding) / 4 = 1024 THEN 'mxbai-embed-large'
    WHEN LENGTH(embedding) / 4 = 1536 THEN 'OpenAI small'
    WHEN LENGTH(embedding) / 4 = 3072 THEN 'OpenAI large'
  END as provider,
  COUNT(*) as count
FROM message_embeddings;
```

---

## ğŸ”„ Re-Indexing to Change Providers

Want to upgrade from Transformers.js to Ollama?

```bash
# 1. Install Ollama and pull model
ollama pull mxbai-embed-large

# 2. Re-index (automatically uses new provider)
User: "Re-index my conversations"

# Old embeddings (384d) replaced with new (1024d)
# Semantic search now uses better model!
```

---

## ğŸ’¡ Key Takeaways

âœ… **Embeddings are automatic** - No manual step required
âœ… **Provider auto-detection** - Uses best available option
âœ… **Graceful fallback** - Still works without embeddings
âœ… **Transparent status** - Tool reports what happened
âœ… **Upgrade anytime** - Just re-index with better provider

---

## ğŸ› Troubleshooting

### "Semantic search returns empty results"

**Check embedding status:**
```
User: "Re-index conversations"
Look for: "âœ… Semantic search enabled" or "âš ï¸ unavailable"
```

**If unavailable:**
1. Install Ollama: `brew install ollama`
2. Pull model: `ollama pull mxbai-embed-large`
3. Re-index: User asks Claude to index again

### "Embeddings taking too long"

**Solution**: Embeddings are generated in batches of 32
- Transformers.js: ~2-3 messages/sec
- Ollama: ~10-15 messages/sec
- OpenAI: ~100-500 messages/sec (batch API)

For 3000 messages:
- Transformers.js: ~15 minutes
- Ollama: ~3-5 minutes
- OpenAI: ~30 seconds

### "Database is empty after indexing"

**This was a bug in v0.1.0!**
- Fixed in v0.2.0
- MCP server was silently failing to generate embeddings
- Now reports status clearly

---

**Last Updated**: 2025-11-05
**Version**: 0.2.0
